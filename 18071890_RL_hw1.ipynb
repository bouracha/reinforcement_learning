{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18071890_RL_hw1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bouracha/reinforcement_learning/blob/master/18071890_RL_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pYs6LMEbNqoQ"
      },
      "cell_type": "markdown",
      "source": [
        "# RL homework 1\n",
        "**Due date: Monday 4 February 2019, 9am**\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "Name: ***Anthony Bourached***\n",
        "\n",
        "Student number: ***18071890***"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6Sns0IKYNtsA"
      },
      "cell_type": "markdown",
      "source": [
        "**How to submit**\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as studentnumber_RL_hw1.ipynb before the deadline above.\n",
        "\n",
        "Also send a sharable link to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
        "\n",
        "We will process the notebook with a tool similar to Jupyter nbconvert, to turn it into a PDF, while excluding the cells with code (many of which are boilerplate).  If you carefully follow the instructions, the coding you have to do will be printed to output as well, and therefore will show up in the PDF.  Feel free to create a PDF with nbconvert as well, if you want to check whether everything looks okay."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9v_SYckYfv5G"
      },
      "cell_type": "markdown",
      "source": [
        "**Context**\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "**Background reading**\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 and 2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rNuohp44N00i"
      },
      "cell_type": "markdown",
      "source": [
        "**Overview of this assignment**\n",
        "\n",
        "You will use Python to implement several bandit algorithms.\n",
        "\n",
        "You will then run these algorithms on a multi-armed Bernoulli bandit problem, to understand the issue of balancing exploration and exploitation."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ztQEQvnKh2t6"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section with the little triangle on the left of the word **Setup** above."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YzYtxi8Wh5SJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import Useful Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "from functools import partial\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YP97bVN3NuG8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VYxNiGcRxbd0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RovingBandit(object):\n",
        "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities,\n",
        "               success_reward=1., fail_reward=0., change_point=500,\n",
        "               change_is_good=True):\n",
        "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "      change_point: The number of steps before the rewards change.\n",
        "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "    self._change_point = change_point\n",
        "    self._change_is_good = change_is_good\n",
        "    self._number_of_steps_so_far = 0\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    self._number_of_steps_so_far += 1\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    \n",
        "    if self._number_of_steps_so_far == self._change_point:\n",
        "      # After some number of steps, the rewards are inverted\n",
        "      #\n",
        "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
        "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
        "      #            - 1984, Orwell (1949).\n",
        "      reward_dif = (self._s - self._f)\n",
        "      if self._change_is_good:\n",
        "        self._f = self._s + reward_dif\n",
        "      else:\n",
        "        self._s -= reward_dif\n",
        "        self._f += reward_dif\n",
        "      \n",
        "      # Recompute expected values when the rewards change\n",
        "      ps = np.array(self._probs)\n",
        "      self._values = ps * self._s + (1 - ps) * self._f\n",
        "\n",
        "    return reward\n",
        "  \n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "  \n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DU7KGFJ0DN-H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"Smoothing function for plotting.\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def one_hot(array, depth):\n",
        "  \"\"\"Multi-dimensional one-hot.\"\"\"\n",
        "  a = np.array(array)\n",
        "  x = a.flatten()\n",
        "  b = np.eye(depth)[x, :depth]\n",
        "  return b.reshape(a.shape + (depth,))\n",
        "\n",
        "def plot(algs, plot_data, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 3\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(8, 3*n_rows))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
        "\n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg in current_algs:\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.6, label=alg.name)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.6, label=alg.name)[0]\n",
        "          plt.fill_between(range(len(m)), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if p.opt_values is not None:\n",
        "        plt.plot(p.opt_values[current_algs[0].name][0], '--k', label='optimal')\n",
        "\n",
        "      ax.set_axis_bgcolor('white')\n",
        "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "      ax.spines[\"top\"].set_visible(False)\n",
        "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
        "      ax.spines[\"right\"].set_visible(False)\n",
        "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
        "      ax.get_xaxis().tick_bottom()\n",
        "      ax.get_yaxis().tick_left()\n",
        "\n",
        "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
        "      \n",
        "      if p.log_plot:\n",
        "        start, end = calculate_lims(data, p.log_plot)\n",
        "        start = np.floor(np.log10(start))\n",
        "        end = np.ceil(np.log10(end))\n",
        "        ticks = [_*10**__\n",
        "                 for _ in [1., 2., 3., 5.]\n",
        "                 for __ in [-2., -1., 0.]]\n",
        "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
        "                  for _ in [1, 2, 3, 5]\n",
        "                  for __ in [-2, -1, 0]]\n",
        "        plt.yticks(ticks, labels)\n",
        "      plt.ylim(calculate_lims(data, p.log_plot))\n",
        "      plt.locator_params(axis='x', nbins=4)\n",
        "      \n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  action_dict = {}\n",
        "  regret_dict = {}\n",
        "  optimal_value_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    action_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "\n",
        "    for _rep in range(repetitions):\n",
        "      bandit = bandit_constructor()\n",
        "      alg.reset()\n",
        "\n",
        "      action = None\n",
        "      reward = None\n",
        "      for _step in range(number_of_steps):\n",
        "        action = alg.step(action, reward)\n",
        "        reward = bandit.step(action)\n",
        "        regret = bandit.regret(action)\n",
        "        optimal_value = bandit.optimal_value()\n",
        "\n",
        "        reward_dict[alg.name][_rep, _step] = reward\n",
        "        action_dict[alg.name][_rep, _step] = action\n",
        "        regret_dict[alg.name][_rep, _step] = regret\n",
        "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
        "\n",
        "  return reward_dict, action_dict, regret_dict, optimal_value_dict\n",
        "\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=30,\n",
        "                 success_reward=1., fail_reward=0.,\n",
        "                 bandit_class=BernoulliBandit):\n",
        "\n",
        "  success_probabilities = np.arange(0.25, 0.75 + 1e-6, 0.5/(number_of_arms - 1))\n",
        "\n",
        "  bandit_constructor = partial(bandit_class,\n",
        "                               success_probabilities=success_probabilities,\n",
        "                               success_reward=success_reward,\n",
        "                               fail_reward=fail_reward)\n",
        "  rewards, actions, regrets, opt_values = run_experiment(\n",
        "      bandit_constructor, agents, repetitions, number_of_steps)\n",
        "\n",
        "  smoothed_rewards = {}\n",
        "  for agent, rs in rewards.items():\n",
        "    smoothed_rewards[agent] = np.array(rs)\n",
        "\n",
        "  PlotData = collections.namedtuple('PlotData',\n",
        "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
        "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
        "               opt_values=opt_values, log_plot=False),\n",
        "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
        "               log_plot=True),\n",
        "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
        "               log_plot=False),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, repetitions)\n",
        "\n",
        "def calculate_lims(data, log_plot=False):\n",
        "  y_min = np.min(data)\n",
        "  y_max = np.max(data)\n",
        "  diff = y_max - y_min\n",
        "  if log_plot:\n",
        "    y_min = 0.9*y_min\n",
        "    y_max = 1.1*y_max\n",
        "  else:\n",
        "    y_min = y_min - 0.05*diff\n",
        "    y_max = y_max + 0.05*diff\n",
        "  return y_min, y_max\n",
        "\n",
        "def argmax(array):\n",
        "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
        "  return np.random.choice(np.flatnonzero(array == array.max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WPrdvRUQIi6k"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic Agents\n",
        "\n",
        "This section contains a few agents we implemented for you.  Please read the code and make sure you understand the interface of the agents."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5EeGHFjslTeI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
        "  random. The 'previous_action' argument of 'step' is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\n",
        "    \n",
        "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
        "    bandits `arm` is just another word for `action`.)\n",
        "    \"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'random'\n",
        "\n",
        "  def step(self, unused_previous_action, unused_reward):\n",
        "    \"\"\"Returns a random action.\n",
        "    \n",
        "    The inputs are ignored, but this function still requires an action and a\n",
        "    reward, to have the same interface as other agents who may use these inputs\n",
        "    to learn.\n",
        "    \"\"\"\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gG5kYiXmGaUv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Greedy(object):\n",
        "  \"\"\"A greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', by always\n",
        "  selecting the action with the highest estimated value.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'greedy'\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "    \n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input).  Then, the greedy action\n",
        "    is returned according to the current (updated) statistics.\n",
        "    \n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "    if previous_action is not None:\n",
        "      self._counts[previous_action] += 1.\n",
        "      r = reward\n",
        "      q_a = self._estimates[previous_action]\n",
        "      n_a = self._counts[previous_action]\n",
        "      self._estimates[previous_action] += (r - q_a)/n_a\n",
        "    action = argmax(self._estimates)\n",
        "    \n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets all the statistics (estimated rewards and counts) to zero.\n",
        "    \n",
        "    This is also used to initialise these statistics on initialisation.\n",
        "    \"\"\"\n",
        "    self._estimates = np.zeros((self._number_of_arms,))\n",
        "    self._counts = np.zeros((self._number_of_arms,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TgoUGGM8Gaty",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy(object):\n",
        "  \"\"\"An epsilon-greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
        "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
        "  with probability `epsilon` it samples an action uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, number_of_arms, epsilon=0.1):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._epsilon = epsilon\n",
        "    self.name = '$\\epsilon$-greedy with $\\epsilon$ = {}'.format(epsilon)\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "    \n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input), and returns an action.\n",
        "    The action is either uniformly random (with probability epsilon), or greedy\n",
        "    (with probability 1 - epsilon).\n",
        "    \n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "    if previous_action is not None:\n",
        "      self._counts[previous_action] += 1.\n",
        "      r = reward\n",
        "      q_a = self._estimates[previous_action]\n",
        "      n_a = self._counts[previous_action]\n",
        "      self._estimates[previous_action] += (r - q_a)/n_a\n",
        "    if np.random.random() < self._epsilon:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    else:\n",
        "      action = argmax(self._estimates)\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets all the statistics (estimated rewards and counts) to zero.\n",
        "    \n",
        "    This is also used to initialise these statistics on initialisation.\n",
        "    \"\"\"\n",
        "    self._estimates = np.zeros((self._number_of_arms,))\n",
        "    self._counts = np.zeros((self._number_of_arms,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qvkgCezuKsbd"
      },
      "cell_type": "markdown",
      "source": [
        "# REINFORCE: softmax policies\n",
        "\n",
        "Derive a REINFORCE policy-gradient method for a multi-armed bandit problem.\n",
        "\n",
        "The policy is a softmax on action preferences, with temperature T:\n",
        "$$\\pi(a) = \\frac{\\exp(p(a)/T)}{\\sum_b \\exp(p(b)/T)}\\,.$$\n",
        "\n",
        "The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "In the next text field (under **Q1**), write down the update function to the preferences for all actions $\\{a_1, \\ldots, a_n\\}$, given the preferences $p_t(\\cdot)$, and assuming you selected a specific action $A_t \\in \\{a_1, \\ldots, a_n\\}$ and received a reward of $R_t$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s7Z28uMgK3Zh"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "**[5 pts]** **Instructions**: please provide answer **in this text cell*:\n",
        "\n",
        "*Answer here*\n",
        "\\begin{align*}\n",
        "p_{t+1}(a)\n",
        "& = \\ldots\n",
        "&& \\text{for $a = A_t$} \\\\\n",
        "p_{t+1}(b)\n",
        "& = \\ldots\n",
        "&& \\text{for all $b \\ne A_t$}\n",
        "\\end{align*}"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "q-Dcpb6bLGEY"
      },
      "cell_type": "markdown",
      "source": [
        "# REINFORCE: different parametrization\n",
        "\n",
        "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice.\n",
        "\n",
        "Consider for instance the `square-max` policy parameterization, on action preferences $p(\\cdot)$:\n",
        "\n",
        "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
        "\n",
        "Derive a REINFORCE policy-gradient method for updating the preferences under this policy distribution.\n",
        "\n",
        "As before, the action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "In the next text field (under **Q2**), write down the update function to the preferences for all actions $\\{a_1, \\ldots, a_n\\}$,\n",
        "given the preferences $p_t(\\cdot)$, and assuming you selected a specific action $A_t \\in \\{a_1, \\ldots, a_n\\}$ and received a reward of $R_t$."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UnNZp5EkLEXN"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "**[5 pts]** **Instructions**: please provide answer in markdown below **within this text cell**.\n",
        "\n",
        "*Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fzpb_dGVjT0O"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent implementations"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jBHsuFyapu5r"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "All agents should be in pure Python.\n",
        "\n",
        "You cannot use TensorFlow or other AutoDiff packages to compute gradients.\n",
        "\n",
        "It is fine to use TF-Eager or Numpy for other calculations.\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "**`step(self, previous_action, reward)`:**\n",
        "\n",
        "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "**`reset(self)`:**\n",
        "\n",
        "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
        "\n",
        "Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "**`__init__(self, number_of_arms, *args)`:**\n",
        "\n",
        "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args.\n",
        "\n",
        "The `name` attribute set by the agent's constructor should be unique (e.g., 'greedy', 'ucb', etc.)\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TDTyvlZsvSQq"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q3\n",
        "**[10 pts]** You should **implement** an agent that **explores with UCB**. \n",
        "\n",
        "The **whole agent** needs to be contained in the code cell below.  Do **not** use multiple cells."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0gnFb-urm_7C",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/rl1_3.py\n",
        "class UCB(object):\n",
        "\n",
        "  def __init__(self, number_of_arms):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = 'ucb'\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lcZik4OfCrK5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't modify this cell\n",
        "print('** Answer 3')\n",
        "print('[10 pts]')\n",
        "f = open('/tmp/rl1_3.py')\n",
        "print(''.join(f.readlines()))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "enKI7uNjI1Ym"
      },
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "\n",
        "**[10 pts]** You should **implement a softmax REINFORCE agent with** and **without a baseline**.\n",
        "\n",
        "The **whole agent** needs to be contained in the code cell below.  Do **not** use multiple cells.\n",
        "\n",
        "The constructor should take as arguments:\n",
        "*  `number_of_arms`, (int), number of discrete actions,\n",
        "*  `baseline`: (boolean, default False), whether or not to use an average-reward baseline,\n",
        "*  `step_size`: (float, default 0.1), the step-size for the updates,\n",
        "*  `temperature`: (float, default 1.0), the temperature of the softmax policy, .\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tAOdBBadI3jD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/rl1_4.py\n",
        "class REINFORCE(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_arms, step_size=0.1, baseline=False, temperature=1.0):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._lr = step_size\n",
        "    self._baseline = baseline\n",
        "    self._temperature = temperature\n",
        "    self.name = 'reinforce, baseline: {}, temperature: {}'.format(\n",
        "        baseline, temperature)\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ruuv50gdCvoR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't modify this cell\n",
        "print('** Answer 4')\n",
        "print('[10 pts]')\n",
        "f = open('/tmp/rl1_4.py')\n",
        "print(''.join(f.readlines()))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1jZsPzCmDxAh"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xQkk8sMxE0N4"
      },
      "cell_type": "markdown",
      "source": [
        "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "GsNBHNZtHCPe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 1: Bernoulli bandit\n",
        "%%capture experiment1\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(number_of_arms),\n",
        "    Greedy(number_of_arms),\n",
        "    EpsilonGreedy(number_of_arms, 0.1),\n",
        "    EpsilonGreedy(number_of_arms, 0.01),\n",
        "    UCB(number_of_arms),\n",
        "    REINFORCE(number_of_arms),\n",
        "    REINFORCE(number_of_arms, baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oKM_20gtxrMI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MWyaMm-sjz9a"
      },
      "cell_type": "markdown",
      "source": [
        "## Q5\n",
        "(Answer inline in the markdown below each question, **within this text cell**.)\n",
        "\n",
        "**[5pts]** Name the best and worst algorithms, and explain (with one or two sentences each) why these are best and worst.\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which algorithms are guaranteed to have linear total regret?\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which algorithms are guaranteed to have logarithmic total regret?\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[5pts]** Which of the $\\epsilon$-greedy algorithms performs best?  Which should perform best in the long run?\n",
        "\n",
        "*Answer here.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0YO5NDaPGDsp"
      },
      "cell_type": "markdown",
      "source": [
        "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
        "\n",
        "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "7cvJf4WzmJXK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
        "%%capture experiment2\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5GOe5RDsnj4J"
      },
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        "**[10pts]** Explain which algorithms improved from the changed rewards, and why.\n",
        "\n",
        "(Use at most two sentences per algorithm and feel free to combine explanations for different algorithms where possible).\n",
        "\n",
        "*Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8f10aCVC0SKU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 3: Non-stationary bandit\n",
        "# Reward on `failure` changes from 0 to +2.\n",
        "# Reward on `success` remains at +1.\n",
        "\n",
        "%%capture experiment3\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "agents = [\n",
        "    EpsilonGreedy(number_of_arms, 0.1),\n",
        "    EpsilonGreedy(number_of_arms, 0.01),\n",
        "    UCB(number_of_arms),\n",
        "    REINFORCE(number_of_arms, baseline=True),\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(RovingBandit, change_is_good=True)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yT_mZxCIAfg9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment 4: Non-stationary bandit\n",
        "# Reward on `failure` changes from 0 to +1.\n",
        "# Reward on `success` changes from +1 to 0.\n",
        "\n",
        "%%capture experiment4\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "agents = [\n",
        "    EpsilonGreedy(number_of_arms, 0.1),\n",
        "    EpsilonGreedy(number_of_arms, 0.01),\n",
        "    UCB(number_of_arms),\n",
        "    REINFORCE(number_of_arms, baseline=True),\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(RovingBandit, change_is_good=False)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CX11Qwqd1Dpm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment3.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s703_VCICCTL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment4.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5x84zO7DK2_t"
      },
      "cell_type": "markdown",
      "source": [
        "## Q7\n",
        "\n",
        "Observe the reward and regret curves above.  After 500 steps, the rewards change: `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2, which means that the identity of the optimal action changes.\n",
        "\n",
        "Below, we ask for explanations.  Answer each question briefly, *using at most three sentences per question*.\n",
        "\n",
        "**[10 pts]** Look at the current-regret curve for UCB in **experiment 3**.  Explain why it looks the way it does after the change in rewards.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "**[6 pts]** Explain why the current-regret curves for $\\epsilon$-greedy look different than that of UCB in **experiment 3**.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "**[6 pts]** Explain why the current-regret curve for REINFORCE looks different than that of UCB in **experiment 3**.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "**[6 pts]** Compare the current-regret curve for UCB in **experiment 3** to the current regret curve of UCB in **experiment 4**.  Explain why they differ.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "**[6 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify the $\\epsilon$-greedy algorithm implemented above to deal with that better?  Be specific and concise.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "**[6 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
        "\n",
        "*Answer here*"
      ]
    }
  ]
}