{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bouracha/reinforcement_learning/blob/master/RL_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pYs6LMEbNqoQ"
      },
      "cell_type": "markdown",
      "source": [
        "# RL homework 2\n",
        "**Due date: 25 February 2019, 9:00am **"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6Sns0IKYNtsA"
      },
      "cell_type": "markdown",
      "source": [
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<student_id>_ucldm_rl2.ipynb`** before the deadline above."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9v_SYckYfv5G"
      },
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning algorithms for sequential decision problems.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rNuohp44N00i"
      },
      "cell_type": "markdown",
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ztQEQvnKh2t6"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qB0tQ4aiAaIu"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YzYtxi8Wh5SJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6NDhSYfSDcCC"
      },
      "cell_type": "markdown",
      "source": [
        "### Set options"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ps5OnkPmDbMX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-colorblind')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ALrRR76eAd6u"
      },
      "cell_type": "markdown",
      "source": [
        "### A grid world"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YP97bVN3NuG8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = -100  # wall\n",
        "G = 100  # goal\n",
        "GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
        "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
        "])\n",
        "\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    if self._layout[new_y, new_x] == W:  # wall\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cOu9RZY3AkF1"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6EttQGJ1n5Zn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += reward\n",
        "\n",
        "    return mean_reward/float(number_of_steps)\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(4, 3, 5)\n",
        "  v = np.max(q, axis=-1)\n",
        "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "  \n",
        "  # Plot arrows:\n",
        "  plt.subplot(4, 3, 11)\n",
        "  plot_values(grid, grid==0, vmax=1)\n",
        "  for row in range(len(grid)):\n",
        "    for col in range(len(grid[0])):\n",
        "      if grid[row][col] == 0:\n",
        "        argmax_a = np.argmax(q[row, col])\n",
        "        if argmax_a == 0:\n",
        "          x = col\n",
        "          y = row + 0.5\n",
        "          dx = 0\n",
        "          dy = -0.8\n",
        "        if argmax_a == 1:\n",
        "          x = col - 0.5\n",
        "          y = row\n",
        "          dx = 0.8\n",
        "          dy = 0\n",
        "        if argmax_a == 2:\n",
        "          x = col\n",
        "          y = row - 0.5\n",
        "          dx = 0\n",
        "          dy = 0.8\n",
        "        if argmax_a == 3:\n",
        "          x = col + 0.5\n",
        "          y = row\n",
        "          dx = -0.8\n",
        "          dy = 0\n",
        "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fzpb_dGVjT0O"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement agents\n",
        "\n",
        "Each agent, should implement a step function:\n",
        "\n",
        "### `step(self, reward, discount, next_observation, ...)`:\n",
        "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" in the update, because $\\gamma = 0$ (for whatever definition of $v$ is appropriate---for instance, $v(s)$ could be defined in terms of action values estimates that we are learning, for instance by $v(s) = \\max_a q(s, a)$).  So, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
        "\n",
        "In this assignment, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.\n",
        "\n",
        "All agents should be in pure Python - so you cannot use TensorFlow to, e.g., compute gradients.  Using `numpy` is fine.\n",
        "\n",
        "### A note on the initial action\n",
        "In our experiments the helper functions above will execute the action `0` (which corresponds to `up`) as the initial action to begin the run loop of the experiment.  This initial action is only executed once, and the beginning of the very first episode---not at the beginning of each episode.\n",
        "\n",
        "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._last_action = 0\n",
        "  (...)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t0Z5IgXfU2Qw"
      },
      "cell_type": "markdown",
      "source": [
        "### A random agent\n",
        "\n",
        "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Mf64o3b3U6A4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(number_of_actions)\n",
        "    return next_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UaGeLcsvixmt"
      },
      "cell_type": "markdown",
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SlFuWFzIi5uB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grid = Grid()\n",
        "grid.plot_grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B8oKd0oyvNcH"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q1: Implement TD learning\n",
        "**[5 pts]** Implement an agent that behaves randomly, but that _on-policy_ estimates state values $v(s)$, using one-step TD learning with a step size $\\alpha=0.1$.\n",
        "\n",
        "Also implement the method `get_values(self)` that returns the vector of all state values (one value per state).\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement `get_values` and `step`.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  Hint: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Hyo1QCD4kePY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile randomTD.py\n",
        "# don't remove the line above\n",
        "\n",
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self._values = np.zeros(number_of_states)\n",
        "    self._state = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cKGy7DA1qc-Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "###### Execute this cell after implementing the algorithm #####\n",
        "######### in the previous cell. Don't modify this cell ########\n",
        "###############################################################\n",
        "print('**Q1: TD')\n",
        "print('[5 pts]')\n",
        "f = open('randomTD.py')\n",
        "q_string = ''.join(f.readlines())\n",
        "exec(compile(q_string, 'td', 'exec'))\n",
        "print(q_string)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oaMmp1lDgpUG"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
        "\n",
        "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange below)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N0ZoYwgZfho2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "agent = RandomTD(grid._layout.size, 4, grid.get_obs())\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "v = agent.get_values()\n",
        "plot_values(GRID_LAYOUT, v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-300, vmax=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wxc_Sx7og4JH"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2: Policy iteration\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following questions.\n",
        "\n",
        "**[5 pts]** The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MKfA7ifHvO-M"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q3: Implement a general Q-learning agent\n",
        "**[20 pts]** Implement a  **general Q-learning** agent that learns action values from experience.   The agent must act according to an $\\epsilon$-greedy policy over its action values.  It must be configurable so as to update action values according to any of **Sarsa**, **Expected Sarsa**, **Q-learning**,  and **double Q-learning**.\n",
        "\n",
        "The `__init__` must accept two functions `target_policy` and `behaviour_policy` as arguments.   The function `behaviour_policy(action_values)` should map `action_values` to a single action. \n",
        "\n",
        "For instance, the random policy can be implemented as:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return np.random.randint(len(action_values))\n",
        "```\n",
        "and $\\epsilon$-greedy can be implemented using the `epsilon_greedy` helper function:\n",
        "```\n",
        "def behaviour_policy(action_values):\n",
        "  return epsilon_greedy(action_values, epsilon=0.1)\n",
        "```\n",
        "\n",
        "The target policy is defined by a function `target_policy(action_values, action)`, which should return **a vector** with one probability per action.  The `action` argument is used to be able to do Sarsa: in addition to the action values, the function will also get the action as selected by the behaviour so that it can return a one hot vector for just the selected action in the Sarsa case.  So, the target policy for Sarsa would look like this:\n",
        "```\n",
        "def one_hot(index, max_index):\n",
        "  # returns a vector of length `max_index` with zeros in all elements,\n",
        "  # except the element at position `index`, which is equal to one.\n",
        "  np.eye(max_index)[index]\n",
        "\n",
        "def target_policy(action_values, action):\n",
        "  return one_hot(action)\n",
        "```\n",
        "As another example, a random target policy is:\n",
        "```\n",
        "def target_policy(action_values, unused_action):\n",
        "  number_of_actions = len(action_values)\n",
        "  return np.ones((number_of_actions,))/number_of_actions\n",
        "```\n",
        "\n",
        "The `__init__` must also accept  a `double` boolean flag. Note that this is compatible with any choice of `target_policy` and `behaviour_policy`. For instance, if the `target_policy` is the policy described above for Sarsa and `double=True`, the algorithm should implement **double Sarsa**. Note that we then need two action-value functions.\n",
        "\n",
        "**Note**: the following agent interface is *sufficient* to instantiate any of **Sarsa**, **Expected Sarsa**, **Q-learning**,  and **double Q-learning**:\n",
        "\n",
        "*   `__init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1)`\n",
        "\n",
        "*   `step(self, reward, discount, next_state)`\n",
        "\n",
        "We will mostly use `step_size=0.1`, so make that the default, but allow it to change when it is fed in as an argument.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wq_qf3E_Z7NT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile general_q.py\n",
        "# don't remove the line above\n",
        "\n",
        "class GeneralQ(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    self._s = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    self._last_action = 0  # The very first action is always 0 (=up) in our setup.\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    if self._double:\n",
        "      return (self._q + self._q2)/2\n",
        "    else:\n",
        "      return self._q\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    raise RuntimeError(\"\"\"\\n\\n\n",
        "    *********************************\n",
        "    *** step(...) not implemented ***\n",
        "    *********************************\n",
        "    \"\"\")\n",
        "    ##############################################################\n",
        "    ### REMOVE THE RUNTIME ERROR ABOVE AND IMPLEMENT STEP HERE ###\n",
        "    ##############################################################\n",
        "    return action\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XR3fBTbZkEdt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "###### Execute this cell after implementing the algorithm #####\n",
        "######### in the previous cell. Don't modify this cell ########\n",
        "###############################################################\n",
        "print('**Q3: General Q')\n",
        "print('[10 pts]')\n",
        "f = open('general_q.py')\n",
        "q_string = ''.join(f.readlines())\n",
        "exec(compile(q_string, 'q', 'exec'))\n",
        "print(q_string)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rp5ZE0v5pTgR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Experiment setup\n",
        "epsilon = 0.25\n",
        "step_size = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xQkk8sMxE0N4"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the cells below to train Q-learning, Sarsa, Expected Sarsa, and double Q-learning agents and generate plots.\n",
        "\n",
        "This trains the agents the Grid problem with a step size $\\alpha=\\frac{1}{10}$ and $\\epsilon$-greedy behaviour, with $\\epsilon=\\frac{1}{4}$.\n",
        "\n",
        "The plots below will show action values for each of the actions, as well as a state value defined by $v(s) = \\max_a q(s, a)$."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "GsNBHNZtHCPe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Q-learning\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in xrange(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jZ26VmlsSSmw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sarsa\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  return np.eye(len(q))[a]\n",
        "\n",
        "learned_qs = []\n",
        "for _ in xrange(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mti_FtEeSaqH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Expected Sarsa\n",
        "grid = Grid()\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "def target_policy(q, a):\n",
        "  greedy = np.eye(len(q))[np.argmax(q)]\n",
        "  return greedy - greedy*epsilon + epsilon/4 \n",
        "\n",
        "learned_qs = []\n",
        "for _ in xrange(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=False, step_size=step_size)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-20, vmax=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5DwHqxeZ1rXa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# double Q-learning\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon=0.1)\n",
        "def target_policy(q, a):\n",
        "  # Place equal probability on all actions that achieve the `max` value.\n",
        "  # This is equivalent to `return np.eye(len(q))[np.argmax(q)]` for Q-learning\n",
        "  # But results in slightly lower variance updates for double Q.\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "learned_qs = []\n",
        "for _ in xrange(5):\n",
        "  grid = Grid()\n",
        "  agent = GeneralQ(grid._layout.size, 4, grid.get_obs(), target_policy, \n",
        "                   behaviour_policy, double=True, step_size=1.)\n",
        "  run_experiment(grid, agent, int(1e5))\n",
        "  learned_qs.append(agent.q_values.reshape(grid._layout.shape + (4,)))\n",
        "  \n",
        "avg_qs = sum(learned_qs)/len(learned_qs)\n",
        "plot_action_values(GRID_LAYOUT, avg_qs, vmin=-50, vmax=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LGptHwE23lmP"
      },
      "cell_type": "markdown",
      "source": [
        "## Q4: Analyse results (Part 1)\n",
        "\n",
        "Consider the greedy policy with respect to the estimated values learnt by each of the four agents.\n",
        "\n",
        "**[10 pts]** How and why do the policies found by Q-learning, Sarsa, Expected Sarsa, and double Q-learning differ? Explain notable qualitative differences in at most four sentences.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[10 pts]** Which of the algorithms, out of Q-learning, Sarsa, Expected Sarsa, and double Q-learning with the learning parameters (exploration, step size) as discussed above, will *in general* on average yield higher returns during learning?  You are allowed to specify a partial (rather than a full) ordering over the algorithms, but try to be as specfic as you can. Explain your answer in at most four sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "e07_wN_CTZx4"
      },
      "cell_type": "markdown",
      "source": [
        "## Q5: Target Q-learning\n",
        "\n",
        "Consider a new algorithm which we will refer to as **target Q-learning**. The target Q-learning algorithm applies Q-learning updates in the form \n",
        "$\\ \\ q(S_t, A_t) \\leftarrow R_{t+1} + \\gamma \\max_a q'(S_{t+1}, a) \\ \\ $  \n",
        "where the values $q'$ have been pre-trained by running $\\epsilon$-greedy Expected Sarsa, and are then held fixed throughout training.\n",
        "\n",
        "Imagine now to train both Q-learning and target Q-learning on the Grid problem.  In both Q-learning and target Q-learning the behaviour policy will select actions based on the same $\\epsilon$-greedy policy that was used to pre-train the $q'$ values, but using the current action-value estimates (not the fixed target values $q'$).\n",
        "\n",
        "**[10 pts]** Explain concisely which of the two algorithms will perform better and why. (If you're tempted to answer 'it depends', be sure to be clear what you think it depends on, and why. Don't be vague: point could be subtracted for including irrelevant or false statements, even if the correct answer is also given.)\n",
        "\n",
        "> *Answer here*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R4oXm0_omzWJ"
      },
      "cell_type": "markdown",
      "source": [
        "## The second grid world\n",
        "\n",
        "In the grid world below, the agent gets a reward of -1 on each step in the top corridor, except if it bumps into a black wall: then the reward is $-100$.\n",
        "\n",
        "There are two versions of the domain, that differ only in terms of the rewards received in the *vertical* corridor: in the deterministic variant, when `noisy=False`, the reward there is also $-1$ per step.  In the stochastic variance, when `noisy=True`, the per step reward is either $-12$ or $+10$ with equal probability, regardless of whether the agent bump into a wall or makes another move.  In both cases, the rewards in the top corridor are deterministic (when conditioned on the agent's action).\n",
        "\n",
        "The discount is $\\gamma=1$.  Each episode ends only when the agent enters a terminal state, denoted with $\\mathrm{T}$.  The agent then starts a new episode from the starting state $\\mathrm{S}$.  (Equivalently, you can think of the agent transitioning to $\\mathrm{S}$ instead of to $\\mathrm{T}$, with a discount of $\\gamma_t=0$ on that specific time step.)\n",
        "\n",
        "When the agent bumps into one walls (darker grey or black), it stays in the same state. For instance, moving *UP* from the start state yields a reward of $-1$ and the subsequent state will be the same state. Similarly, moving *DOWN* at the bottom of the vertical corridor will yield a reward of $-12$ or $+10$, and the agent will remain in that state."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dNQulr3I9wQm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = -100\n",
        "X = -1\n",
        "GRID2_LAYOUT = np.array([\n",
        "  [W, W, W, X, X, X, X, X],\n",
        "  [W, 1, 0, 0, 0, 0, 1, X],\n",
        "  [W, W, X, 0, X, X, X, X],\n",
        "  [W, W, X, 0, X, X, X, X],\n",
        "  [W, W, X, 0, X, X, X, X],\n",
        "  [W, W, X, 0, X, X, X, X],\n",
        "  [W, W, X, X, X, X, X, X],\n",
        "])\n",
        "\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "class Grid2(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (1, 3)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    if action == UP:\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == RIGHT:\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == DOWN:\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == LEFT:\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    discount = 1.\n",
        "\n",
        "    if reward == W or reward == X:  # wall\n",
        "      new_state = (y, x)  # bounced\n",
        "    elif reward == 0:  # empty cell\n",
        "      reward = -1\n",
        "    else:  # a goal\n",
        "      reward = 0\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    if self._noisy:\n",
        "      if y > 1:\n",
        "        reward = np.random.choice([-12, 10])  # -1 on average, but noisy\n",
        "    \n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    x = self._layout\n",
        "    plt.imshow((x + np.sqrt(x**2 + 1))**0.4,  # transform for pretty plotting\n",
        "               interpolation=\"nearest\", cmap='bone')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(3, 1, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(1, 1, r\"$\\mathbf{T}$\", ha='center', va='center')\n",
        "    plt.text(6, 1, r\"$\\mathbf{T}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "y21eRxSsyN5W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grid = Grid2()\n",
        "grid.plot_grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8LKhFNDF3jCJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Configs\n",
        "epsilon = 0.5\n",
        "step_size = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZoZD09jt6ow0"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the cells below to train Sarsa / Expected Sarsa / Q-learning / double Q-learning on a noisy version of the grid-world.\n",
        "\n",
        "The cell below will run 10 repetitions of each experiment corresponding to all combinations of algorithm and environment, for the four algorithms named above and for the two variants of the grid world (with, and without noisy rewards in the vertical corridor)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7jACDxo7xIzu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Policies.\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, epsilon)\n",
        "\n",
        "def greedy(q):\n",
        "  max_q = np.max(q)\n",
        "  pi = np.array([1. if qi == max_q else 0. for qi in q])\n",
        "  return pi / sum(pi)\n",
        "\n",
        "q_target_policy = lambda q, _: greedy(q)\n",
        "\n",
        "def sarsa_target_policy(q, a):\n",
        "  return np.eye(len(q))[a]\n",
        "\n",
        "def expected_sarsa_target_policy(q, _):\n",
        "  return (1 - epsilon)*greedy(q) + epsilon/len(q) \n",
        "\n",
        "plt_num = 0\n",
        "fig = plt.figure(figsize=(10, 9))\n",
        "fig.subplots_adjust(wspace=0.3, hspace=0.5)\n",
        "\n",
        "  \n",
        "for noisy in [False, True]:\n",
        "  mean_rewards = dict(\n",
        "      q_learning=[],\n",
        "      double_q_learning=[],\n",
        "      sarsa = [],\n",
        "      expected_sarsa = [])\n",
        "\n",
        "  final_rewards = dict(\n",
        "      q_learning=[],\n",
        "      double_q_learning=[],\n",
        "      sarsa = [],\n",
        "      expected_sarsa = [])\n",
        "\n",
        "  from functools import partial\n",
        "  agent_fn = partial(GeneralQ,\n",
        "                     number_of_states=grid._layout.size,\n",
        "                     number_of_actions=4,\n",
        "                     initial_state=grid.get_obs(),\n",
        "                     behaviour_policy=behaviour_policy,\n",
        "                     step_size=step_size)\n",
        "  for _ in range(20):\n",
        "    # Instantiate all 4 agents.\n",
        "    agents = dict(\n",
        "      q_learning=agent_fn(target_policy=q_target_policy, double=False),\n",
        "      sarsa=agent_fn(target_policy=sarsa_target_policy, double=False),\n",
        "      expected_sarsa=agent_fn(target_policy=expected_sarsa_target_policy, double=False),\n",
        "      double_q_learning=agent_fn(target_policy=q_target_policy, double=True),\n",
        "    )\n",
        "\n",
        "    # Run an experiment with each of the agents.\n",
        "    for name, agent in agents.items():\n",
        "      grid = Grid2(noisy=noisy)\n",
        "      mean_rewards[name].append(run_experiment(grid, agent, int(1e4)))\n",
        "\n",
        "      # evaluate greedy policy\n",
        "      agent._behaviour_policy = lambda q: np.argmax(q)\n",
        "      agent._step_size = 0.\n",
        "      final_rewards[name].append(run_experiment(grid, agent, int(1e4)))  \n",
        "\n",
        "\n",
        "  # Plot performance across runs.\n",
        "  for rewards, label in [(mean_rewards, \"average reward during learning\\nnoisy rewards: {}\".format(noisy)),\n",
        "                         (final_rewards, \"reward of greedy policy after learning\\nnoisy rewards: {}\".format(noisy))]:\n",
        "    plt_num += 1\n",
        "    plt.subplot(2, 2, plt_num)\n",
        "    plt.boxplot([\n",
        "        rewards[alg] for alg in\n",
        "        [\"q_learning\", \"sarsa\", \"expected_sarsa\", \"double_q_learning\"]\n",
        "    ])\n",
        "    plt.xticks([1, 2, 3, 4], [\"Q-learning\", \"Sarsa\", \"Expected Sarsa\", \"Double Q-learning\"], rotation=60, size=12, ha='right')\n",
        "    plt.ylabel(label, size=12)\n",
        "    ax = plt.gca()\n",
        "    ax.set_axis_bgcolor('white')\n",
        "    ax.grid(0)\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "itgvt-pQpKFD"
      },
      "cell_type": "markdown",
      "source": [
        "## Q6: Analyse results (Part 2)\n",
        "\n",
        "The plots above show \n",
        " * **left column**: the distributions of average rewards per step over all learning steps (i.e., a single number per experiment) over 20 experiments per algorithm,\n",
        " * **right column**: the distributions of average rewards per step when executing the greedy policy after learning over 20 experiments per algorithm,\n",
        "\n",
        "and\n",
        "\n",
        " * **top row**: when the rewards are deterministic: $r=-1$ per step, expect when bumping into a black wall: $r=-100$, and except on termination: $r=0$,\n",
        " * **bottom row**: when the rewards in the vertical corridor are randomly either $r=-12$ or $r=+10$, with equal probability.\n",
        " \n",
        " Answer the questions below.   Be complete, but concise.  Remember that points can be deducted for irrelevant or false statements, even if the answer also includes all of the true statements we were looking for."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a_Jj_RoHbxfF"
      },
      "cell_type": "markdown",
      "source": [
        "**[16 pts]** Look at the top row.  Note which two algorithms performed best during learning (left), and which two performed best after learning (right).  How and why does the performance differ, and how is it the same, between the left and right plots? Explain the observed performances for all eight experiments in the top row, and explicitly contrast and explain notable observed differences.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[12 pts]** Compare all the results in the top row with the results in the bottom row.  For all eight experiments in the bottom row, explain the observed performance, and mention explicitly why the performance is notably different or why it is the same as in the top row.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[12 pts]** Suppose we run the same experiments with a fixed step size of $\\alpha=1$. You can ignore double Q-learning for this question.  For each of the other three algorithms, Q-learning, Sarsa, and Expected Sarsa, what would happen in both the deterministic and the noisy domain?  For each of the six combinations of algorithm and domain, briefly describe how the action values behave when we would run the experiments for a very long time (so you can ignore any effects that only matter at the beginning of learning, such as the initial value estimates).\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ESLiywh5pFJl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fin"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}